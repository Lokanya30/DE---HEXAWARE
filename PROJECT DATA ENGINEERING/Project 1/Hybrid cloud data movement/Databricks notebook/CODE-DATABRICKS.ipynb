{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "868a6ec9-0742-4e6a-90b1-691d005d6eae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+------+--------------------+--------------------+----------+----------------+-------------+----------+--------------------+\n",
      "|FresherID|     FullName|Gender|                 DOB|               Email|     Phone|         Address|Qualification|Department|         JoiningDate|\n",
      "+---------+-------------+------+--------------------+--------------------+----------+----------------+-------------+----------+--------------------+\n",
      "|        1|  Amit Sharma|  Male|2000-02-15 00:00:...|amit.sharma@examp...|9876543210|    Delhi, India|   B.Tech CSE|        IT|2025-01-10 00:00:...|\n",
      "|        2|   Neha Verma|Female|1999-11-25 00:00:...|neha.verma@exampl...|9876501234|   Mumbai, India|       MBA HR|        HR|2025-01-10 00:00:...|\n",
      "|        3|  Rahul Mehta|  Male|2001-03-12 00:00:...|rahul.mehta@examp...|9876123456|     Pune, India|        B.Com|   Finance|2025-01-10 00:00:...|\n",
      "|        4|   Sneha Iyer|Female|2000-07-08 00:00:...|sneha.iyer@exampl...|9876234567|  Chennai, India|      B.Sc IT|        IT|2025-01-15 00:00:...|\n",
      "|        5|Karthik Reddy|  Male|1999-05-20 00:00:...|karthik.reddy@exa...|9876345678|Hyderabad, India|          MCA|        IT|2025-01-15 00:00:...|\n",
      "+---------+-------------+------+--------------------+--------------------+----------+----------------+-------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- FresherID: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- DOB: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Phone: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Qualification: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- JoiningDate: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\n",
    "  \"fs.azure.account.key.lokanyastrg.blob.core.windows.net\",\n",
    "  \"\"AZURE_STORAGE_KEY\"\")\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"wasbs://filestrg@lokanyastrg.blob.core.windows.net/dbo.Employee1.txt\")\n",
    "\n",
    "df.show(5)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c596d13e-90ad-4409-a015-37945aa3deae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----------------+--------------------+------+\n",
      "|                 DOB|     Phone|          Address|               Email|Gender|\n",
      "+--------------------+----------+-----------------+--------------------+------+\n",
      "|2000-02-15 00:00:...|9876543210|     Delhi, India|amit.sharma@examp...|  Male|\n",
      "|1999-11-25 00:00:...|9876501234|    Mumbai, India|neha.verma@exampl...|Female|\n",
      "|2001-03-12 00:00:...|9876123456|      Pune, India|rahul.mehta@examp...|  Male|\n",
      "|2000-07-08 00:00:...|9876234567|   Chennai, India|sneha.iyer@exampl...|Female|\n",
      "|1999-05-20 00:00:...|9876345678| Hyderabad, India|karthik.reddy@exa...|  Male|\n",
      "|2000-09-18 00:00:...|9876456789|     Kochi, India|priya.nair@exampl...|Female|\n",
      "|2001-01-05 00:00:...|9876567890| Bangalore, India|suresh.kumar@exam...|  Male|\n",
      "|2000-04-10 00:00:...|9876678901|    Jaipur, India|ananya.gupta@exam...|Female|\n",
      "|1999-12-22 00:00:...|9876789012| Ahmedabad, India|ravi.patel@exampl...|  Male|\n",
      "|2001-06-14 00:00:...|9876890123|   Lucknow, India|meera.joshi@examp...|Female|\n",
      "|2000-08-30 00:00:...|9876901234|     Surat, India|arjun.desai@examp...|  Male|\n",
      "|1999-10-09 00:00:...|9876012345|Trivandrum, India|divya.menon@examp...|Female|\n",
      "|2001-02-18 00:00:...|9876123450|    Kanpur, India|varun.singh@examp...|  Male|\n",
      "|2000-03-25 00:00:...|9876234501|Chandigarh, India|ritika.malhotra@e...|Female|\n",
      "|1999-07-12 00:00:...|9876345012|    Indore, India|akash.jain@exampl...|  Male|\n",
      "|2000-11-01 00:00:...|9876450123|     Delhi, India|shreya.kapoor@exa...|Female|\n",
      "|2001-09-22 00:00:...|9876567891|     Noida, India|vikram.chauhan@ex...|  Male|\n",
      "|1999-05-19 00:00:...|9876678902|    Nagpur, India|pooja.rathi@examp...|Female|\n",
      "|2000-01-30 00:00:...|9876789013|    Bhopal, India|rohan.khanna@exam...|  Male|\n",
      "|2001-12-05 00:00:...|9876890124|  Amritsar, India|simran.kaur@examp...|Female|\n",
      "+--------------------+----------+-----------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cleaning: Remove nulls, filter rows, rename columns\n",
    "#Select Required Columns and Basic Cleaning\n",
    "selected_cols = [\"DOB\", \"Phone\", \"Address\", \"Email\", \"Gender\"]\n",
    "df_selected = df.select(selected_cols)\n",
    "\n",
    "df_cleaned = df_selected.na.drop()\n",
    "df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1dac1e1-a2a8-4d9b-9091-9f6988359d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----------------+--------------------+------+\n",
      "|                 DOB|     Phone|          Address|               Email|Gender|\n",
      "+--------------------+----------+-----------------+--------------------+------+\n",
      "|1999-11-25 00:00:...|9876501234|    Mumbai, India|neha.verma@exampl...|Female|\n",
      "|2000-07-08 00:00:...|9876234567|   Chennai, India|sneha.iyer@exampl...|Female|\n",
      "|2000-09-18 00:00:...|9876456789|     Kochi, India|priya.nair@exampl...|Female|\n",
      "|2000-04-10 00:00:...|9876678901|    Jaipur, India|ananya.gupta@exam...|Female|\n",
      "|2001-06-14 00:00:...|9876890123|   Lucknow, India|meera.joshi@examp...|Female|\n",
      "|1999-10-09 00:00:...|9876012345|Trivandrum, India|divya.menon@examp...|Female|\n",
      "|2000-03-25 00:00:...|9876234501|Chandigarh, India|ritika.malhotra@e...|Female|\n",
      "|2000-11-01 00:00:...|9876450123|     Delhi, India|shreya.kapoor@exa...|Female|\n",
      "|1999-05-19 00:00:...|9876678902|    Nagpur, India|pooja.rathi@examp...|Female|\n",
      "|2001-12-05 00:00:...|9876890124|  Amritsar, India|simran.kaur@examp...|Female|\n",
      "+--------------------+----------+-----------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering: Only Female employees\n",
    "df_female = df_cleaned.filter(df_cleaned[\"Gender\"] == \"Female\")\n",
    "\n",
    "df_female.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "208685f9-bcea-45a6-95f1-0c724d8dddb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----------------+--------------------+------+\n",
      "|                 DOB|     Phone|             City|               Email|Gender|\n",
      "+--------------------+----------+-----------------+--------------------+------+\n",
      "|2000-02-15 00:00:...|9876543210|     Delhi, India|amit.sharma@examp...|  Male|\n",
      "|1999-11-25 00:00:...|9876501234|    Mumbai, India|neha.verma@exampl...|Female|\n",
      "|2001-03-12 00:00:...|9876123456|      Pune, India|rahul.mehta@examp...|  Male|\n",
      "|2000-07-08 00:00:...|9876234567|   Chennai, India|sneha.iyer@exampl...|Female|\n",
      "|1999-05-20 00:00:...|9876345678| Hyderabad, India|karthik.reddy@exa...|  Male|\n",
      "|2000-09-18 00:00:...|9876456789|     Kochi, India|priya.nair@exampl...|Female|\n",
      "|2001-01-05 00:00:...|9876567890| Bangalore, India|suresh.kumar@exam...|  Male|\n",
      "|2000-04-10 00:00:...|9876678901|    Jaipur, India|ananya.gupta@exam...|Female|\n",
      "|1999-12-22 00:00:...|9876789012| Ahmedabad, India|ravi.patel@exampl...|  Male|\n",
      "|2001-06-14 00:00:...|9876890123|   Lucknow, India|meera.joshi@examp...|Female|\n",
      "|2000-08-30 00:00:...|9876901234|     Surat, India|arjun.desai@examp...|  Male|\n",
      "|1999-10-09 00:00:...|9876012345|Trivandrum, India|divya.menon@examp...|Female|\n",
      "|2001-02-18 00:00:...|9876123450|    Kanpur, India|varun.singh@examp...|  Male|\n",
      "|2000-03-25 00:00:...|9876234501|Chandigarh, India|ritika.malhotra@e...|Female|\n",
      "|1999-07-12 00:00:...|9876345012|    Indore, India|akash.jain@exampl...|  Male|\n",
      "|2000-11-01 00:00:...|9876450123|     Delhi, India|shreya.kapoor@exa...|Female|\n",
      "|2001-09-22 00:00:...|9876567891|     Noida, India|vikram.chauhan@ex...|  Male|\n",
      "|1999-05-19 00:00:...|9876678902|    Nagpur, India|pooja.rathi@examp...|Female|\n",
      "|2000-01-30 00:00:...|9876789013|    Bhopal, India|rohan.khanna@exam...|  Male|\n",
      "|2001-12-05 00:00:...|9876890124|  Amritsar, India|simran.kaur@examp...|Female|\n",
      "+--------------------+----------+-----------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Renaming column\n",
    "df_renamed = df_cleaned.withColumnRenamed(\"Address\", \"City\")\n",
    "df_renamed.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80c8f12a-417c-4f7d-af98-7ee5dbfed264",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "selected_cols = [\"FullName\", \"DOB\", \"Phone\", \"Address\", \"Email\", \"Gender\", \"JoiningDate\"]\n",
    "df_selected = df.select(selected_cols)\n",
    "df_cleaned = df_selected.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "915eb6a8-5f02-458a-a943-8050fa292b3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-08-24 19:26:39.984\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `City` cannot be resolved. Did you mean one of the following? [`DOB`, `Email`, `Phone`, `Gender`, `Address`]. SQLSTATE: 42703;\\n'Project [FullName#171, DOB#173, Phone#175, 'City, Email#174, Gender#172, JoiningDate#179]\\n+- Relation [FresherID#170,FullName#171,Gender#172,DOB#173,Email#174,Phone#175,Address#176,Qualification#177,Department#178,JoiningDate#179] csv\\n\\n\\nJVM stacktrace:\\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:504)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:179)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:489)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\\n\\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\\n\\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\\n\\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:416)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:416)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:254)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:254)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\\n\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\\n\\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\\n\\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\\n\\tat scala.util.Try$.apply(Try.scala:213)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\\n\\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:108)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\\n\\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\\n\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\n\\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\\n\\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:106)\\n\\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:137)\\n\\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:104)\\n\\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:66)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\\n\\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\\n\\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\\n\\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:66)\\n\\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:51)\\n\\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:258)\\n\\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:50)\\n\\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:109)\\n\\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:801)\\n\\tat grpc_shaded.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\\n\\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\\n\\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\\n\\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\\n\\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\\n\\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\\n\\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\\n\\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:381)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\\n\\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\\n\\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:337)\\n\\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:537)\\n\\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:337)\\n\\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\\n\\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\\n\\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\\n\\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\\n\\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\\n\\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\\n\\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\\n\\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\\n\\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:336)\\n\\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:349)\\n\\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:329)\\n\\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:381)\\n\\tat grpc_shaded.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:351)\\n\\tat grpc_shaded.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\\n\\tat grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\\n\\tat grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\\n\\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\\n\\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\\n\\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\\n\\tat java.lang.Thread.run(Thread.java:840)\", \"context\": {\"file\": \"<command-4910146995477624>, line 2 in cell [14]\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `City` cannot be resolved. Did you mean one of the following? [`DOB`, `Email`, `Phone`, `Gender`, `Address`]. SQLSTATE: 42703;\\n'Project [FullName#171, DOB#173, Phone#175, 'City, Email#174, Gender#172, JoiningDate#179]\\n+- Relation [FresherID#170,FullName#171,Gender#172,DOB#173,Email#174,Phone#175,Address#176,Qualification#177,Department#178,JoiningDate#179] csv\\n\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `City` cannot be resolved. Did you mean one of the following? [`DOB`, `Email`, `Phone`, `Gender`, `Address`]. SQLSTATE: 42703;\\\\n\\\\'Project [FullName#171, DOB#173, Phone#175, \\\\'City, Email#174, Gender#172, JoiningDate#179]\\\\n+- Relation [FresherID#170,FullName#171,Gender#172,DOB#173,Email#174,Phone#175,Address#176,Qualification#177,Department#178,JoiningDate#179] csv\\\\n\\\", grpc_status:13, created_time:\\\"2025-08-24T19:26:39.97116368+00:00\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_analyze\", \"file\": \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", \"line\": \"1736\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"437\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1177\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1003\"}]}}\n",
      "{\"ts\": \"2025-08-24 19:26:40.037\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `City` cannot be resolved. Did you mean one of the following? [`DOB`, `Email`, `Phone`, `Gender`, `Address`]. SQLSTATE: 42703;\\n'Project [FullName#171, DOB#173, Phone#175, 'City, Email#174, Gender#172, JoiningDate#179]\\n+- Relation [FresherID#170,FullName#171,Gender#172,DOB#173,Email#174,Phone#175,Address#176,Qualification#177,Department#178,JoiningDate#179] csv\\n\\n\\nJVM stacktrace:\\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:504)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:179)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:489)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\\n\\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\\n\\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\\n\\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:416)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:416)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:254)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:254)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\\n\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\\n\\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\\n\\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\\n\\tat scala.util.Try$.apply(Try.scala:213)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\\n\\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:108)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\\n\\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\\n\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\n\\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\\n\\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:106)\\n\\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformNADrop(SparkConnectPlanner.scala:484)\\n\\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:215)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$7(SessionHolder.scala:614)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:630)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$5(SessionHolder.scala:613)\\n\\tat scala.Option.getOrElse(Option.scala:189)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:611)\\n\\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:185)\\n\\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.transformRelation$1(SparkConnectAnalyzeHandler.scala:119)\\n\\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:130)\\n\\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:104)\\n\\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:66)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\\n\\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\\n\\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\\n\\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:66)\\n\\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:51)\\n\\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:258)\\n\\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:50)\\n\\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:109)\\n\\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:801)\\n\\tat grpc_shaded.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\\n\\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\\n\\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\\n\\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\\n\\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\\n\\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\\n\\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\\n\\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:381)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\\n\\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\\n\\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:337)\\n\\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:537)\\n\\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:337)\\n\\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\\n\\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\\n\\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\\n\\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\\n\\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\\n\\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\\n\\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\\n\\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\\n\\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:336)\\n\\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:349)\\n\\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:329)\\n\\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:381)\\n\\tat grpc_shaded.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:351)\\n\\tat grpc_shaded.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\\n\\tat grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\\n\\tat grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\\n\\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\\n\\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\\n\\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\\n\\tat java.lang.Thread.run(Thread.java:840)\", \"context\": {\"file\": \"<command-4910146995477624>, line 2 in cell [14]\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `City` cannot be resolved. Did you mean one of the following? [`DOB`, `Email`, `Phone`, `Gender`, `Address`]. SQLSTATE: 42703;\\n'Project [FullName#171, DOB#173, Phone#175, 'City, Email#174, Gender#172, JoiningDate#179]\\n+- Relation [FresherID#170,FullName#171,Gender#172,DOB#173,Email#174,Phone#175,Address#176,Qualification#177,Department#178,JoiningDate#179] csv\\n\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-08-24T19:26:40.025404298+00:00\\\", grpc_status:13, grpc_message:\\\"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `City` cannot be resolved. Did you mean one of the following? [`DOB`, `Email`, `Phone`, `Gender`, `Address`]. SQLSTATE: 42703;\\\\n\\\\'Project [FullName#171, DOB#173, Phone#175, \\\\'City, Email#174, Gender#172, JoiningDate#179]\\\\n+- Relation [FresherID#170,FullName#171,Gender#172,DOB#173,Email#174,Phone#175,Address#176,Qualification#177,Department#178,JoiningDate#179] csv\\\\n\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_analyze\", \"file\": \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", \"line\": \"1736\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"437\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1177\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1003\"}]}}\n",
      "{\"ts\": \"2025-08-24 19:26:40.092\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `City` cannot be resolved. Did you mean one of the following? [`DOB`, `Email`, `Phone`, `Gender`, `Address`]. SQLSTATE: 42703;\\n'Project [FullName#171, DOB#173, Phone#175, 'City, Email#174, Gender#172, JoiningDate#179]\\n+- Relation [FresherID#170,FullName#171,Gender#172,DOB#173,Email#174,Phone#175,Address#176,Qualification#177,Department#178,JoiningDate#179] csv\\n\\n\\nJVM stacktrace:\\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:504)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:179)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:489)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\\n\\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\\n\\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\\n\\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:416)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:416)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:254)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:254)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\\n\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\\n\\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\\n\\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\\n\\tat scala.util.Try$.apply(Try.scala:213)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\\n\\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:108)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\\n\\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\\n\\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\\n\\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\\n\\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:106)\\n\\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformNADrop(SparkConnectPlanner.scala:484)\\n\\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:215)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$7(SessionHolder.scala:614)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:630)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$5(SessionHolder.scala:613)\\n\\tat scala.Option.getOrElse(Option.scala:189)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:611)\\n\\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:185)\\n\\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:172)\\n\\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformWithColumnsRenamed(SparkConnectPlanner.scala:1485)\\n\\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:232)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$7(SessionHolder.scala:614)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:630)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$5(SessionHolder.scala:613)\\n\\tat scala.Option.getOrElse(Option.scala:189)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:611)\\n\\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:185)\\n\\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.transformRelation$1(SparkConnectAnalyzeHandler.scala:119)\\n\\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:130)\\n\\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:104)\\n\\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:66)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\\n\\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\\n\\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\\n\\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:66)\\n\\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:51)\\n\\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:258)\\n\\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:50)\\n\\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:109)\\n\\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:801)\\n\\tat grpc_shaded.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\\n\\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\\n\\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\\n\\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\\n\\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\\n\\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\\n\\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\\n\\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:381)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\\n\\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\\n\\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:337)\\n\\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:537)\\n\\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:337)\\n\\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\\n\\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\\n\\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\\n\\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\\n\\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\\n\\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\\n\\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\\n\\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\\n\\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:336)\\n\\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:349)\\n\\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:329)\\n\\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:381)\\n\\tat grpc_shaded.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:351)\\n\\tat grpc_shaded.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\\n\\tat grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\\n\\tat grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\\n\\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\\n\\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\\n\\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\\n\\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\\n\\tat java.lang.Thread.run(Thread.java:840)\", \"context\": {\"file\": \"<command-4910146995477624>, line 2 in cell [14]\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `City` cannot be resolved. Did you mean one of the following? [`DOB`, `Email`, `Phone`, `Gender`, `Address`]. SQLSTATE: 42703;\\n'Project [FullName#171, DOB#173, Phone#175, 'City, Email#174, Gender#172, JoiningDate#179]\\n+- Relation [FresherID#170,FullName#171,Gender#172,DOB#173,Email#174,Phone#175,Address#176,Qualification#177,Department#178,JoiningDate#179] csv\\n\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2025-08-24T19:26:40.077020874+00:00\\\", grpc_status:13, grpc_message:\\\"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `City` cannot be resolved. Did you mean one of the following? [`DOB`, `Email`, `Phone`, `Gender`, `Address`]. SQLSTATE: 42703;\\\\n\\\\'Project [FullName#171, DOB#173, Phone#175, \\\\'City, Email#174, Gender#172, JoiningDate#179]\\\\n+- Relation [FresherID#170,FullName#171,Gender#172,DOB#173,Email#174,Phone#175,Address#176,Qualification#177,Department#178,JoiningDate#179] csv\\\\n\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_analyze\", \"file\": \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", \"line\": \"1736\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"437\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1177\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1003\"}]}}\n"
     ]
    }
   ],
   "source": [
    "selected_cols = [\"FullName\", \"DOB\", \"Phone\", \"City\", \"Email\", \"Gender\", \"JoiningDate\"]\n",
    "df_selected = df.select(selected_cols)\n",
    "df_cleaned = df_selected.na.drop()\n",
    "df_renamed = df_cleaned.withColumnRenamed(\"Address\", \"City\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1338150-8140-430e-b862-26e85a18eec8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+----------+----------------+--------------------+------+--------------------+\n",
      "|     FullName|                 DOB|     Phone|            City|               Email|Gender|         JoiningDate|\n",
      "+-------------+--------------------+----------+----------------+--------------------+------+--------------------+\n",
      "|  Amit Sharma|2000-02-15 00:00:...|9876543210|    Delhi, India|amit.sharma@examp...|  Male|2025-01-10 00:00:...|\n",
      "|   Neha Verma|1999-11-25 00:00:...|9876501234|   Mumbai, India|neha.verma@exampl...|Female|2025-01-10 00:00:...|\n",
      "|  Rahul Mehta|2001-03-12 00:00:...|9876123456|     Pune, India|rahul.mehta@examp...|  Male|2025-01-10 00:00:...|\n",
      "|   Sneha Iyer|2000-07-08 00:00:...|9876234567|  Chennai, India|sneha.iyer@exampl...|Female|2025-01-15 00:00:...|\n",
      "|Karthik Reddy|1999-05-20 00:00:...|9876345678|Hyderabad, India|karthik.reddy@exa...|  Male|2025-01-15 00:00:...|\n",
      "+-------------+--------------------+----------+----------------+--------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "+-------------+--------------------+----------+----------------+--------------------+------+--------------------+---------+\n",
      "|     FullName|                 DOB|     Phone|            City|               Email|Gender|         JoiningDate|Seniority|\n",
      "+-------------+--------------------+----------+----------------+--------------------+------+--------------------+---------+\n",
      "|  Amit Sharma|2000-02-15 00:00:...|9876543210|    Delhi, India|amit.sharma@examp...|  Male|2025-01-10 00:00:...|     0.62|\n",
      "|   Neha Verma|1999-11-25 00:00:...|9876501234|   Mumbai, India|neha.verma@exampl...|Female|2025-01-10 00:00:...|     0.62|\n",
      "|  Rahul Mehta|2001-03-12 00:00:...|9876123456|     Pune, India|rahul.mehta@examp...|  Male|2025-01-10 00:00:...|     0.62|\n",
      "|   Sneha Iyer|2000-07-08 00:00:...|9876234567|  Chennai, India|sneha.iyer@exampl...|Female|2025-01-15 00:00:...|     0.61|\n",
      "|Karthik Reddy|1999-05-20 00:00:...|9876345678|Hyderabad, India|karthik.reddy@exa...|  Male|2025-01-15 00:00:...|     0.61|\n",
      "+-------------+--------------------+----------+----------------+--------------------+------+--------------------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Select relevant columns including 'Address'\n",
    "selected_cols = [\"FullName\", \"DOB\", \"Phone\", \"Address\", \"Email\", \"Gender\", \"JoiningDate\"]\n",
    "df_selected = df.select(selected_cols)\n",
    "df_cleaned = df_selected.na.drop()\n",
    "\n",
    "# Step 2: Rename 'Address' to 'City'\n",
    "df_renamed = df_cleaned.withColumnRenamed(\"Address\", \"City\")\n",
    "\n",
    "# Step 3: Access 'City' only in df_renamed onwards (not df_cleaned)\n",
    "df_renamed.show(5)\n",
    "\n",
    "# Step 4: Perform transformations, e.g. add Seniority column\n",
    "from pyspark.sql.functions import datediff, current_date, round\n",
    "df_transformed = df_renamed.withColumn(\n",
    "    \"Seniority\",\n",
    "    round(datediff(current_date(), df_renamed[\"JoiningDate\"]) / 365.0, 2)\n",
    ")\n",
    "\n",
    "df_transformed.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ad821f8-7713-48a6-a0b5-540faca016fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 7: Save processed output back to Blob storage as CSV\n",
    "df_transformed.write.mode(\"overwrite\").option(\"header\",\"true\").csv(\"wasbs://filestrg@lokanyastrg.blob.core.windows.net/processed/employee_processed/\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Projectcode",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
